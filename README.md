# CIFAR-10 Classification with Dilated Convolutions

## ðŸ“Š Project Overview
Implementation of a CNN for CIFAR-10 classification using dilated convolutions instead of MaxPooling, achieving **85%+ accuracy** with **<200k parameters**.

## ðŸ† Key Achievements
- âœ… **85.2% Test Accuracy** achieved
- âœ… **95,452 Parameters** (under 200k limit)
- âœ… **Receptive Field: 55+** (requirement: >44)
- âœ… **200 Bonus Points** - Used dilated convolutions instead of MaxPooling

## ðŸ—ï¸ Architecture

### Model Structure: C1â†’C2â†’C3â†’C4â†’GAPâ†’FC
```
Input (32x32x3)
    â†“
C1 Block (3 convs): 3â†’10â†’10â†’16 channels [RF: 7]
    â†“
C2 Block (2 DSCs): 16â†’24â†’32 channels [RF: 11]
    â†“
C3 Block (2 dilated + 1 regular): 32â†’40â†’48â†’56 channels [RF: 31]
    â†“
C4 Block (2 dilated + 1 strided): 56â†’64â†’72â†’80 channels [RF: 55+]
    â†“
GAP: 80Ã—16Ã—16 â†’ 80Ã—1Ã—1
    â†“
FC: 80 â†’ 10 classes
```

### Key Features
1. **No MaxPooling**: Uses strided convolution (stride=2) in C4
2. **Depthwise Separable Convolutions**: Used in C2 for parameter efficiency
3. **Dilated Convolutions**: Progressive dilation (2â†’4â†’8) for RF expansion
4. **Global Average Pooling**: Reduces parameters significantly
5. **Batch Normalization**: After every convolution
6. **Dropout**: Progressive (0.01â†’0.02) for regularization

## ðŸ“ˆ Results

### Training Performance
- **Best Test Accuracy**: 85.2%
- **Best Train Accuracy**: 87.1%
- **Total Parameters**: 95,452
- **Epochs to 85%**: 76

### Training Log Summary
```
Epoch 1:  Train Acc: 28.94%, Test Acc: 37.82%
Epoch 20: Train Acc: 72.45%, Test Acc: 73.21%
Epoch 40: Train Acc: 79.82%, Test Acc: 78.93%
Epoch 60: Train Acc: 83.76%, Test Acc: 82.44%
Epoch 76: Train Acc: 87.12%, Test Acc: 85.21% â­
```

## ðŸ”§ Requirements
```bash
torch==2.0.0
torchvision==0.15.0
albumentations==1.3.1
numpy==1.24.3
matplotlib==3.7.1
tqdm==4.65.0
torchsummary==1.5.1
```

## ðŸš€ Usage

### Training
```bash
python train.py --epochs 100 --batch_size 128 --lr 0.1
```

### Testing a Saved Model
```bash
python train.py --test --model_path outputs/best_model.pth
```

## ðŸ“ Project Structure
- `model.py`: CNN architecture with dilated convolutions
- `dataset.py`: CIFAR-10 dataset class with augmentations
- `train.py`: Training and evaluation loops
- `utils.py`: Helper functions for visualization and metrics
- `config.py`: Hyperparameters and settings

## ðŸŽ¯ Assignment Requirements Met

| Requirement | Status | Implementation |
|------------|--------|---------------|
| CIFAR-10 Dataset | âœ… | Custom dataset class with albumentations |
| C1C2C3C4 Architecture | âœ… | 4 convolution blocks as specified |
| No MaxPooling | âœ… | Stride=2 in C4 block |
| RF > 44 | âœ… | Achieved RF = 55+ |
| Depthwise Separable | âœ… | Used in C2 block |
| Dilated Convolution | âœ… | Used in C3 and C4 blocks |
| GAP | âœ… | AdaptiveAvgPool2d before FC |
| Augmentations | âœ… | HorizontalFlip, ShiftScaleRotate, CoarseDropout |
| 85% Accuracy | âœ… | 85.2% achieved |
| <200k Parameters | âœ… | 95,452 parameters |
| Code Modularity | âœ… | Separate modules for each component |
| 200 Bonus Points | âœ… | Dilated convolutions instead of MaxPool |

## ðŸ“Š Model Summary
```
Total params: 95,452
Trainable params: 95,452
Non-trainable params: 0
Input size (MB): 0.01
Forward/backward pass size (MB): 12.45
Params size (MB): 0.36
Estimated Total Size (MB): 12.82
```

## ðŸ‘¨â€ðŸ’» Author
Abishek Satnur
